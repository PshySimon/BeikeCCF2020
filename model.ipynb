{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "uuid": "d3de79b9-ee2f-40eb-bb8c-603c34110fc0"
   },
   "outputs": [],
   "source": [
    "# 首先表示出一个文档，然后来衡量他们的相似度\n",
    "from transformers import BertTokenizer, BertConfig, BertModel\n",
    "from DataIter import DataIter\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "class Config:\n",
    "    def __init__(self):\n",
    "        self.bert_path = \"/home/featurize/Ernie\"\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(self.bert_path)\n",
    "        self.max_length = 100\n",
    "        self.input_size = self.tokenizer.vocab_size\n",
    "        self.emb_size = 300\n",
    "        self.hidden_size = 256\n",
    "        self.dropout = 0.2\n",
    "        self.num_classes = 2\n",
    "        self.device = torch.device(\"cuda\") \\\n",
    "            if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "        self.batch_size = 32\n",
    "        self.num_epochs = 10\n",
    "        self.learning_rate = 2e-5\n",
    "        self.weight_decay = 2e-3\n",
    "        self.adam_epsilon = 1e-8\n",
    "        self.warmup_steps = 0\n",
    "        self.filter_sizes = [2,3,4,5]\n",
    "        self.num_filters = 128\n",
    "        self.num_layers = 2\n",
    "        self.max_question_len = 27\n",
    "        self.max_answer_len = 65"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "a1bf14aa-787a-4517-aadd-c49409c239fd"
   },
   "outputs": [],
   "source": [
    "# 设置种子\n",
    "\n",
    "torch.manual_seed(2020)\n",
    "np.random.seed(2020)\n",
    "torch.manual_seed(2020)\n",
    "torch.cuda.manual_seed_all(2020)\n",
    "torch.backends.cudnn.deterministic = True  # cudnn 使用确定性算法，保证每次结果一样\n",
    "\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "3667197a-df84-4534-9017-fc71ad7b0a5b"
   },
   "outputs": [],
   "source": [
    "# 孪生网络，没啥用\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.FC_mult = nn.Sequential(\n",
    "            nn.Linear(self.bert_config.hidden_size, config.hidden_size),\n",
    "            nn.BatchNorm1d(config.hidden_size, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.FC_minus = nn.Sequential(\n",
    "            nn.Linear(self.bert_config.hidden_size, config.hidden_size),\n",
    "            nn.BatchNorm1d(config.hidden_size, 1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.final = nn.Linear(config.hidden_size * 2, 1)\n",
    "\n",
    "        \n",
    "    def forward(self, text1, text2, mask1, mask2):\n",
    "        text1 = self.bert(text1, attention_mask=mask1)[0]\n",
    "        text2 = self.bert(text2, attention_mask=mask2)[0]\n",
    "        u = text1[:,-1,:].clone()\n",
    "        v = text2[:,-1,:].clone()\n",
    "        minus = u-v\n",
    "        mult = u*v\n",
    "        interation_feature = torch.cat([self.FC_mult(mult),self.FC_minus(minus)], 1)\n",
    "        out = self.final(interation_feature)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "57bcd2bd-86f5-4596-84e7-6ec6bf31d52a"
   },
   "outputs": [],
   "source": [
    "# Bert-TextCNN\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        # 然后是提取特征\n",
    "        self.convs = nn.ModuleList(\n",
    "            [nn.Conv1d(self.bert_config.hidden_size, config.num_filters,x) for x in config.filter_sizes]\n",
    "        )\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        # 最后都要经过池化层，使得输出为\n",
    "        self.fc = nn.Linear(config.num_filters * len(config.filter_sizes), config.num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def pool(self, out, conv):\n",
    "        out = self.relu(conv(out))\n",
    "        max_pool = nn.MaxPool1d(out.shape[-1])\n",
    "        out = max_pool(out)\n",
    "        out = out.squeeze(2)\n",
    "        return out\n",
    "        \n",
    "    def forward(self,input_ids, attention_masks, token_type_ids):\n",
    "        embedded = self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids)[0]\n",
    "        # embedding = [batch_size, seq_len, emb_dim]\n",
    "        embedded = embedded.permute(0,2,1)\n",
    "        # embedded = [batch_size, seq_len, emb_dim]\n",
    "        output = [self.pool(embedded, conv) for conv in self.convs]\n",
    "        # output = num_filter_sizes * [batch_size, num_filters]\n",
    "        out = torch.cat(output, dim=1)\n",
    "        # out = [batch_size, num_filter_sizes * num_filters]\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "e0bc059f-e239-4af8-aa5d-ea34344b90a7"
   },
   "outputs": [],
   "source": [
    "# Bert\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.fc = nn.Linear(self.bert_config.hidden_size * 2, 2)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self,input_ids, attention_masks, token_type_ids):\n",
    "#         _, pooled = self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids)\n",
    "        last_hidden_state,pooler_output,hidden_states=self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids, output_hidden_states=True)\n",
    "        output = torch.cat((pooler_output,last_hidden_state[:, 0, :]),1)\n",
    "#         out = self.dropout(pooled)\n",
    "        out = self.fc(output)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(torch.nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.cls_token_head = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(self.bert_config.hidden_size * 4, self.bert_config.hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.qa_sep_token_head = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(self.bert_config.hidden_size * 4, self.bert_config.hidden_size),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Dropout(config.dropout),\n",
    "            nn.Linear(self.bert_config.hidden_size * 2, 2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks, token_type_ids):\n",
    "        question_answer_seps = (torch.sum((token_type_ids == 0) * attention_masks, -1) - 1)\n",
    "        _, _, hidden_states = self.bert(input_ids,\n",
    "                                        attention_mask=attention_masks,\n",
    "                                        token_type_ids=token_type_ids,\n",
    "                                        output_hidden_states=True)\n",
    "        hidden_states_cls_embeddings = [x[:, 0] for x in hidden_states[-4:]]\n",
    "        x = torch.cat(hidden_states_cls_embeddings, dim=-1)\n",
    "        x_cls = self.cls_token_head(x)\n",
    "        \n",
    "        # Gather [SEP] hidden states\n",
    "        tmp = torch.arange(0, len(input_ids), dtype=torch.long)\n",
    "        hidden_states_qa_sep_embeddings = [x[tmp, question_answer_seps] for x in hidden_states[-4:]]\n",
    "        x = torch.cat(hidden_states_qa_sep_embeddings, dim=-1)\n",
    "        \n",
    "        x_qa_sep = self.qa_sep_token_head(x)\n",
    "        x = torch.cat([x_cls, x_qa_sep], -1)\n",
    "        x = self.linear(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "dd4bf135-b43c-45f9-af72-a0d53802dda3"
   },
   "outputs": [],
   "source": [
    "# SA-Bert\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.output_weights = nn.Parameter(self.truncated_normal_(torch.randn(self.bert_config.hidden_size, config.num_classes).requires_grad_()))\n",
    "        self.output_bias = nn.Parameter(torch.zeros(config.num_classes).requires_grad_())\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.target_loss_weight = nn.Parameter(torch.tensor([1., 1.]).requires_grad_())\n",
    "        self.bce_loss = nn.BCEWithLogitsLoss(reduction='none')\n",
    "        \n",
    "        \n",
    "    # 截断正态分布\n",
    "    def truncated_normal_(self, tensor,mean=0,std=0.02):\n",
    "        with torch.no_grad():\n",
    "            size = tensor.shape\n",
    "            tmp = tensor.new_empty(size+(4,)).normal_()\n",
    "            valid = (tmp < 2) & (tmp > -2)\n",
    "            ind = valid.max(-1, keepdim=True)[1]\n",
    "            tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "            tensor.data.mul_(std).add_(mean)\n",
    "            return tensor\n",
    "        \n",
    "    def forward(self, input_ids, attention_masks, token_type_ids, labels):\n",
    "        \n",
    "        flagx = (labels>0).float()\n",
    "        flagy = (labels==0).float()\n",
    "        all_target_loss = flagx * self.target_loss_weight[1] + flagy * self.target_loss_weight[0]\n",
    "        # pooled = [batch_size, hidden_size]\n",
    "        _, pooled = self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids)\n",
    "        pooled = self.dropout(pooled)\n",
    "        # out = [batch_size, num_classes]\n",
    "        out = torch.matmul(pooled, self.output_weights)\n",
    "        out = out + self.output_bias\n",
    "        out = torch.sigmoid(out)\n",
    "        # out = [batch_size]\n",
    "        logits = out.squeeze(-1)\n",
    "        loss = self.bce_loss(logits, labels)\n",
    "#         losses = all_target_loss * loss\n",
    "        mean_loss = torch.mean(loss)\n",
    "        return mean_loss, logits\n",
    "    \n",
    "    def predict(self, input_ids, attention_masks, token_type_ids, labels):\n",
    "        # pooled = [batch_size, hidden_size]\n",
    "        _, pooled = self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids)\n",
    "        pooled = self.dropout(pooled)\n",
    "        # out = [batch_size, num_classes]\n",
    "        out = torch.matmul(pooled, self.output_weights)\n",
    "        out = out + self.output_bias\n",
    "        out = torch.sigmoid(out)\n",
    "        logits = out.squeeze(-1)\n",
    "        prob = (logits > 0.5).float()\n",
    "        return prob\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "f81d5e7c-eeb6-4701-8d19-cdc731bf375f"
   },
   "outputs": [],
   "source": [
    "# BertRCNN\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.rnn = nn.LSTM(self.bert_config.hidden_size, config.hidden_size, batch_first=True, bidirectional=True, num_layers=config.num_layers)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.fc = nn.Linear(config.hidden_size*2, config.num_classes)\n",
    "        self.w = nn.Parameter(torch.randn(2*config.hidden_size + self.bert_config.hidden_size, 2 * config.hidden_size))\n",
    "        \n",
    "    def forward(self,input_ids, attention_masks, token_type_ids):\n",
    "        embedded = self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids)[0]\n",
    "        # embedded = [batch_size, seq_len, 768]\n",
    "        out,_ = self.rnn(embedded)\n",
    "        # 将输出和嵌入层连接起来\n",
    "        # out = [batch_size, seq_len, 128*2]\n",
    "        out = torch.cat((out, embedded), dim=2)\n",
    "        # out = [batch_size, seq_len, hidden_size * 2 + emb_dim\n",
    "        out = torch.tanh(torch.matmul(out, self.w))\n",
    "        # out = [batch_size, seq_len, hidden_size * 2\n",
    "        out = out.permute(0,2,1)\n",
    "        # out = [batch_size, hidden_size * 2, seq_len]\n",
    "        out = nn.functional.max_pool1d(out, out.shape[-1]).squeeze(2)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "uuid": "34d554bb-ffd8-4049-94fa-d35005e199c9"
   },
   "outputs": [],
   "source": [
    "# BertRNN_Att\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.rnn = nn.LSTM(self.bert_config.hidden_size, 256, bidirectional=True, num_layers = config.num_layers\n",
    "                           ,batch_first=True)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.tanh1 = nn.Tanh()\n",
    "        self.tanh2 = nn.Tanh()\n",
    "        self.w = nn.Parameter(torch.randn(2 * 256))\n",
    "        self.fc1 = nn.Linear(256*2, 512)\n",
    "        self.fc2 = nn.Linear(512, config.num_classes)\n",
    "        \n",
    "    def forward(self,input_ids, attention_masks, token_type_ids):\n",
    "        embedded = self.bert(input_ids, attention_mask = attention_masks, token_type_ids = token_type_ids)[0]\n",
    "        out,_ = self.rnn(embedded)\n",
    "        # out = [batch_size, seq_len, hidden_size * num_directions]\n",
    "        M = self.tanh1(out)\n",
    "        score = torch.matmul(out, self.w)\n",
    "        att = torch.softmax(score, dim=1).unsqueeze(-1)\n",
    "        # att = [batch_size, seq_len, 1]\n",
    "        out = out * att\n",
    "        # out = [batch_size, seq_len, hidden_size * 2]\n",
    "        out = torch.sum(out, 1)\n",
    "        # out = [batch_size, hidden_size * 2]\n",
    "        out = torch.relu(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.n_use_layer = 4\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "        self.dense1 = nn.Linear(self.bert_config.hidden_size*self.n_use_layer, self.bert_config.hidden_size*self.n_use_layer)\n",
    "        self.dense2 = nn.Linear(self.bert_config.hidden_size*self.n_use_layer, self.bert_config.hidden_size*self.n_use_layer)    \n",
    "        self.classifier = nn.Linear(self.bert_config.hidden_size*self.n_use_layer, config.num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_masks, token_type_ids):\n",
    "\n",
    "        outputs = self.bert(input_ids,\n",
    "                            attention_mask = attention_masks,\n",
    "                            token_type_ids = token_type_ids,\n",
    "                            output_hidden_states=True)\n",
    "        pooled_output = torch.cat([outputs[2][-1*i][:,0] for i in range(1, self.n_use_layer+1)], dim=1)\n",
    "        pooled_output = self.dense1(pooled_output)\n",
    "        pooled_output = self.dense2(pooled_output)\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert_config = BertConfig.from_pretrained(config.bert_path)\n",
    "        self.bert = BertModel.from_pretrained(config.bert_path, config = self.bert_config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.high_dropout = nn.Dropout(0.5)\n",
    "\n",
    "        n_weights = self.bert_config.num_hidden_layers + 1\n",
    "        weights_init = torch.zeros(n_weights).float()\n",
    "        weights_init.data[:-1] = -3\n",
    "        self.layer_weights = torch.nn.Parameter(weights_init)\n",
    "        self.classifier = nn.Linear(self.bert_config.hidden_size, config.num_classes)\n",
    "\n",
    "\n",
    "    def forward(self,input_ids, attention_masks, token_type_ids):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_masks,\n",
    "            token_type_ids=token_type_ids,\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        hidden_layers = outputs[2]\n",
    "        cls_outputs = torch.stack(\n",
    "            [self.dropout(layer[:, 0, :]) for layer in hidden_layers], dim=2\n",
    "        )\n",
    "        cls_output = (torch.softmax(self.layer_weights, dim=0) * cls_outputs).sum(-1)\n",
    "        # multisample dropout (wut): https://arxiv.org/abs/1905.09788\n",
    "        logits = torch.mean(\n",
    "            torch.stack(\n",
    "                [self.classifier(self.high_dropout(cls_output)) for _ in range(5)],\n",
    "                dim=0,\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "d5dd22c5-ac09-43fe-83e8-539994658474"
   },
   "outputs": [],
   "source": [
    "class FGM():\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.backup = {}\n",
    "\n",
    "    def attack(self, epsilon=1., emb_name='word_embeddings'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                self.backup[name] = param.data.clone()\n",
    "                norm = torch.norm(param.grad)\n",
    "                if norm != 0:\n",
    "                    r_at = epsilon * param.grad / norm\n",
    "                    param.data.add_(r_at)\n",
    "\n",
    "    def restore(self, emb_name='word_embeddings'):\n",
    "        # emb_name这个参数要换成你模型中embedding的参数名\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad and emb_name in name:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score,accuracy_score, recall_score\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "def predict(config, model, test_iter):\n",
    "    all_pred = []\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        print(\"\\r 正在预测输出%d/%d\"%(i, len(test_iter)), end=\"\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_masks, token_type_ids = batch\n",
    "            out = model(input_ids, attention_masks, token_type_ids)\n",
    "            y_pred = torch.softmax(out, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "            all_pred.append(y_pred)\n",
    "    print(\"\\r预测完成.\\n\")\n",
    "    return np.concatenate(all_pred)\n",
    "\n",
    "def evaluate(config, model, data_iter):\n",
    "    all_pred = np.array([])\n",
    "    all_true = np.array([])\n",
    "    total_loss = 0.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_masks, token_type_ids, label = batch\n",
    "            out = model(input_ids, attention_masks, token_type_ids)\n",
    "            loss = criterion(out, label)\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(out, dim=-1).float().detach().cpu().numpy()\n",
    "            y_true = label.detach().cpu().numpy()\n",
    "            all_pred = np.append(all_pred, y_pred, axis=0)\n",
    "            all_true = np.append(all_true, y_true, axis=0)\n",
    "    score = f1_score(all_true, all_pred)\n",
    "    accuracy = accuracy_score(all_true, all_pred)\n",
    "    recall = recall_score(all_true, all_pred)\n",
    "    return total_loss / len(data_iter), score, accuracy, recall\n",
    "\n",
    "def train(config, model, train_iter, test_iter, data_iter):\n",
    "\n",
    "    save_path = \"/home/featurize/output/Ernie/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    gkf = StratifiedKFold(n_splits=5, shuffle=True).split(data_iter.train_df.q1, data_iter.train_df.label)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        train_inputs = [train_iter[i][train_idx] for i in range(len(train_iter))]\n",
    "        valid_inputs = [train_iter[i][valid_idx] for i in range(len(train_iter))]\n",
    "        train, dev = data_iter.build_iterator(train_inputs, valid_inputs)\n",
    "        model = Model(config)\n",
    "        model.to(config.device)\n",
    "        fgm = FGM(model)\n",
    "        best_score = 0.\n",
    "        best_loss = float('inf')\n",
    "        stop_steps = 0\n",
    "        early_stop = 2000\n",
    "        flag = False\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': config.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        t_total = len(train) * config.num_epochs\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, eps=config.adam_epsilon)\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=\"O0\")\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=t_total)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(\"*************************************Fold:[{}/{}]**********************************\".format(fold+1, 5))\n",
    "        for epoch in range(config.num_epochs):\n",
    "            epoch_loss = 0.\n",
    "            print(\"EPOCH:[{}/{}]\".format(epoch+1, config.num_epochs))\n",
    "            for i, batch in enumerate(tqdm(train)):\n",
    "                model.train()\n",
    "\n",
    "                input_ids, attention_masks, token_type_ids, label = batch\n",
    "                out = model(input_ids, attention_masks, token_type_ids)\n",
    "                loss = criterion(out, label)\n",
    "                # loss.backward()\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                fgm.attack() # 在embedding上添加对抗扰动\n",
    "                out_adv = model(input_ids, attention_masks, token_type_ids)\n",
    "                loss_adv = criterion(out_adv, label)\n",
    "                with amp.scale_loss(loss_adv, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "                # loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "                fgm.restore() # 恢复embedding参数\n",
    "                epoch_loss += loss.item()\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), 1.0)\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                msg = \"ITER:{}, TRAIN_LOSS:{:.3f}, TRAIN_ACC:{:.2%}, TRAIN_RECALL:{:.2%}, TRAIN_F1:{:.2%}\\n DEV_LOSS:{:.3f},\\\n",
    "                        DEV_ACC:{:.2%}, DEV_RECALL:{:.2%}, DEV_F1:{:.2%},NO_IMPROVEMENT:{}\"\n",
    "                if stop_steps > early_stop:\n",
    "                    print(\"超过{}步没有提升，停止迭代\".format(stop_steps))\n",
    "                    flag = True\n",
    "                    break\n",
    "                if i % 100 == 0:\n",
    "                    y_pred = torch.argmax(out, dim=-1).float().detach().cpu().numpy()\n",
    "                    y_true = label.detach().cpu().numpy()\n",
    "                    score = f1_score(y_true, y_pred)\n",
    "                    accuracy = accuracy_score(y_true, y_pred)\n",
    "                    recall = recall_score(y_true, y_pred)\n",
    "                    dev_loss, dev_score, dev_acc, dev_recall = evaluate(config, model, dev)\n",
    "                    print(msg.format(i, loss.item(), accuracy, recall, score, dev_loss, dev_acc, dev_recall, dev_score, stop_steps))\n",
    "                        \n",
    "                    if dev_score > best_score:\n",
    "                        best_score = dev_score\n",
    "                        stop_steps = 0\n",
    "                        torch.save({'state_dict': model.state_dict()}, '/home/featurize/output/Ernie/checkpoint_cnn{}.pth.tar'.format(fold))\n",
    "                stop_steps += 1\n",
    "            if flag:\n",
    "                break\n",
    "        print(\"*********************************************************************************\")\n",
    "        data_iter._gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "uuid": "0d7010a6-1b44-4bad-ab93-d6ba9c7514ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score,accuracy_score, recall_score\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import get_linear_schedule_with_warmup, AdamW\n",
    "\n",
    "def predict(config, model, test_iter):\n",
    "    all_pred = []\n",
    "    for i, batch in enumerate(test_iter):\n",
    "        print(\"\\r 正在预测输出%d/%d\"%(i, len(test_iter)), end=\"\")\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_masks, token_type_ids = batch\n",
    "            out = model(input_ids, attention_masks, token_type_ids)\n",
    "            y_pred = torch.softmax(out, dim=-1).detach().cpu().numpy()\n",
    "\n",
    "            all_pred.append(y_pred)\n",
    "    print(\"\\r预测完成.\\n\")\n",
    "    return np.concatenate(all_pred)\n",
    "\n",
    "def evaluate(config, model, data_iter):\n",
    "    all_pred = np.array([])\n",
    "    all_true = np.array([])\n",
    "    total_loss = 0.\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    for i, batch in enumerate(data_iter):\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            input_ids, attention_masks, token_type_ids, label = batch\n",
    "            out = model(input_ids, attention_masks, token_type_ids)\n",
    "            loss = criterion(out, label)\n",
    "            total_loss += loss.item()\n",
    "            y_pred = torch.argmax(out, dim=-1).float().detach().cpu().numpy()\n",
    "            y_true = label.detach().cpu().numpy()\n",
    "            all_pred = np.append(all_pred, y_pred, axis=0)\n",
    "            all_true = np.append(all_true, y_true, axis=0)\n",
    "    score = f1_score(all_true, all_pred)\n",
    "    accuracy = accuracy_score(all_true, all_pred)\n",
    "    recall = recall_score(all_true, all_pred)\n",
    "    return total_loss / len(data_iter), score, accuracy, recall\n",
    "\n",
    "def train(config, model, train_iter, test_iter, data_iter):\n",
    "\n",
    "    save_path = \"/home/featurize/output/Ernie/\"\n",
    "    if not os.path.exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    gkf = StratifiedKFold(n_splits=5, shuffle=True).split(data_iter.train_df.q1, data_iter.train_df.label)\n",
    "    for fold, (train_idx, valid_idx) in enumerate(gkf):\n",
    "        train_inputs = [train_iter[i][train_idx] for i in range(len(train_iter))]\n",
    "        valid_inputs = [train_iter[i][valid_idx] for i in range(len(train_iter))]\n",
    "        train, dev = data_iter.build_iterator(train_inputs, valid_inputs)\n",
    "        model = Model(config)\n",
    "        model.to(config.device)\n",
    "        fgm = FGM(model)\n",
    "        best_score = 0.\n",
    "        best_loss = float('inf')\n",
    "        stop_steps = 0\n",
    "        early_stop = 2000\n",
    "        flag = False\n",
    "\n",
    "        no_decay = ['bias', 'LayerNorm.weight']\n",
    "        optimizer_grouped_parameters = [\n",
    "            {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "             'weight_decay': config.weight_decay},\n",
    "            {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
    "        ]\n",
    "        t_total = len(train) * config.num_epochs\n",
    "        optimizer = AdamW(optimizer_grouped_parameters, lr=config.learning_rate, eps=config.adam_epsilon)\n",
    "        scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=config.warmup_steps, num_training_steps=t_total)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        print(\"*************************************Fold:[{}/{}]**********************************\".format(fold+1, 5))\n",
    "        for epoch in range(config.num_epochs):\n",
    "            epoch_loss = 0.\n",
    "            print(\"EPOCH:[{}/{}]\".format(epoch+1, config.num_epochs))\n",
    "            for i, batch in enumerate(tqdm(train)):\n",
    "                model.train()\n",
    "\n",
    "                input_ids, attention_masks, token_type_ids, label = batch\n",
    "                out = model(input_ids, attention_masks, token_type_ids)\n",
    "                loss = criterion(out, label)\n",
    "                loss.backward()\n",
    "                fgm.attack() # 在embedding上添加对抗扰动\n",
    "                out_adv = model(input_ids, attention_masks, token_type_ids)\n",
    "                loss_adv = criterion(out_adv, label)\n",
    "                loss_adv.backward() # 反向传播，并在正常的grad基础上，累加对抗训练的梯度\n",
    "                fgm.restore() # 恢复embedding参数\n",
    "                epoch_loss += loss.item()\n",
    "                optimizer.step()\n",
    "                model.zero_grad()\n",
    "                \n",
    "                msg = \"ITER:{}, TRAIN_LOSS:{:.3f}, TRAIN_ACC:{:.2%}, TRAIN_RECALL:{:.2%}, TRAIN_F1:{:.2%}\\n DEV_LOSS:{:.3f},\\\n",
    "                        DEV_ACC:{:.2%}, DEV_RECALL:{:.2%}, DEV_F1:{:.2%},NO_IMPROVEMENT:{}\"\n",
    "                if stop_steps > early_stop:\n",
    "                    print(\"超过{}步没有提升，停止迭代\".format(stop_steps))\n",
    "                    flag = True\n",
    "                    break\n",
    "                if i % 100 == 0:\n",
    "                    y_pred = torch.argmax(out, dim=-1).float().detach().cpu().numpy()\n",
    "                    y_true = label.detach().cpu().numpy()\n",
    "                    score = f1_score(y_true, y_pred)\n",
    "                    accuracy = accuracy_score(y_true, y_pred)\n",
    "                    recall = recall_score(y_true, y_pred)\n",
    "                    dev_loss, dev_score, dev_acc, dev_recall = evaluate(config, model, dev)\n",
    "                    print(msg.format(i, loss.item(), accuracy, recall, score, dev_loss, dev_acc, dev_recall, dev_score, stop_steps))\n",
    "                        \n",
    "                    if dev_score > best_score:\n",
    "                        best_score = dev_score\n",
    "                        stop_steps = 0\n",
    "                        torch.save({'state_dict': model.state_dict()}, '/home/featurize/output/Ernie/checkpoint_cnn{}.pth.tar'.format(fold))\n",
    "                stop_steps += 1\n",
    "            if flag:\n",
    "                break\n",
    "        print(\"*********************************************************************************\")\n",
    "        data_iter._gc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "uuid": "a276b99c-adbb-447b-9248-0d15f94a5312"
   },
   "outputs": [],
   "source": [
    "config = Config()\n",
    "model = Model(config)\n",
    "model.to(config.device)\n",
    "data_iter = DataIter(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "uuid": "8d754120-c304-47c6-b514-9e79974f4ef2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "53757it [00:53, 1013.35it/s]\n",
      "21585it [00:21, 1018.31it/s]\n"
     ]
    }
   ],
   "source": [
    "test_iter = data_iter.build_test(test = True, is_match=True)\n",
    "train_iter = data_iter.build_examples(data_iter.train_df, is_match=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "08919a57-e916-4e78-bcfb-3fd74316bbbc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Building dataset...\n",
      "WARNING:root:Loading data from storage, it may cost a time.\n",
      "  0%|          | 0/540 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*************************************Fold:[1/5]**********************************\n",
      "EPOCH:[1/10]\n",
      "ITER:0, TRAIN_LOSS:0.815, TRAIN_ACC:40.62%, TRAIN_RECALL:87.50%, TRAIN_F1:42.42%\n",
      " DEV_LOSS:0.719,                        DEV_ACC:43.96%, DEV_RECALL:49.40%, DEV_F1:30.74%,NO_IMPROVEMENT:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 100/540 [01:17<04:36,  1.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:100, TRAIN_LOSS:0.248, TRAIN_ACC:90.62%, TRAIN_RECALL:57.14%, TRAIN_F1:72.73%\n",
      " DEV_LOSS:0.324,                        DEV_ACC:86.52%, DEV_RECALL:59.98%, DEV_F1:69.14%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/540 [02:36<03:35,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:200, TRAIN_LOSS:0.330, TRAIN_ACC:87.50%, TRAIN_RECALL:60.00%, TRAIN_F1:60.00%\n",
      " DEV_LOSS:0.281,                        DEV_ACC:88.07%, DEV_RECALL:75.99%, DEV_F1:76.23%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 300/540 [03:55<02:32,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:300, TRAIN_LOSS:0.323, TRAIN_ACC:84.38%, TRAIN_RECALL:87.50%, TRAIN_F1:73.68%\n",
      " DEV_LOSS:0.261,                        DEV_ACC:88.79%, DEV_RECALL:78.29%, DEV_F1:77.86%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 400/540 [05:14<01:28,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:400, TRAIN_LOSS:0.387, TRAIN_ACC:90.62%, TRAIN_RECALL:77.78%, TRAIN_F1:82.35%\n",
      " DEV_LOSS:0.259,                        DEV_ACC:88.65%, DEV_RECALL:82.24%, DEV_F1:78.49%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 500/540 [06:33<00:25,  1.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:500, TRAIN_LOSS:0.232, TRAIN_ACC:90.62%, TRAIN_RECALL:75.00%, TRAIN_F1:80.00%\n",
      " DEV_LOSS:0.245,                        DEV_ACC:89.32%, DEV_RECALL:80.77%, DEV_F1:79.21%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [07:13<00:00,  1.25it/s]\n",
      "  0%|          | 0/540 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:[2/10]\n",
      "ITER:0, TRAIN_LOSS:0.117, TRAIN_ACC:100.00%, TRAIN_RECALL:100.00%, TRAIN_F1:100.00%\n",
      " DEV_LOSS:0.242,                        DEV_ACC:89.62%, DEV_RECALL:78.75%, DEV_F1:79.26%,NO_IMPROVEMENT:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 100/540 [01:18<04:39,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:100, TRAIN_LOSS:0.126, TRAIN_ACC:96.88%, TRAIN_RECALL:100.00%, TRAIN_F1:96.30%\n",
      " DEV_LOSS:0.253,                        DEV_ACC:89.74%, DEV_RECALL:83.81%, DEV_F1:80.44%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/540 [02:37<03:36,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:200, TRAIN_LOSS:0.270, TRAIN_ACC:90.62%, TRAIN_RECALL:66.67%, TRAIN_F1:57.14%\n",
      " DEV_LOSS:0.247,                        DEV_ACC:89.79%, DEV_RECALL:84.36%, DEV_F1:80.62%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 301/540 [04:09<17:59,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:300, TRAIN_LOSS:0.211, TRAIN_ACC:90.62%, TRAIN_RECALL:75.00%, TRAIN_F1:66.67%\n",
      " DEV_LOSS:0.243,                        DEV_ACC:89.62%, DEV_RECALL:83.81%, DEV_F1:80.26%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▍  | 401/540 [05:23<10:23,  4.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:400, TRAIN_LOSS:0.258, TRAIN_ACC:93.75%, TRAIN_RECALL:60.00%, TRAIN_F1:75.00%\n",
      " DEV_LOSS:0.240,                        DEV_ACC:90.00%, DEV_RECALL:81.97%, DEV_F1:80.49%,NO_IMPROVEMENT:200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 500/540 [06:23<00:24,  1.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:500, TRAIN_LOSS:0.169, TRAIN_ACC:90.62%, TRAIN_RECALL:90.91%, TRAIN_F1:86.96%\n",
      " DEV_LOSS:0.235,                        DEV_ACC:90.16%, DEV_RECALL:83.35%, DEV_F1:81.00%,NO_IMPROVEMENT:300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 540/540 [07:04<00:00,  1.27it/s]\n",
      "  0%|          | 0/540 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:[3/10]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/540 [00:13<2:02:01, 13.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:0, TRAIN_LOSS:0.091, TRAIN_ACC:96.88%, TRAIN_RECALL:100.00%, TRAIN_F1:80.00%\n",
      " DEV_LOSS:0.231,                        DEV_ACC:89.93%, DEV_RECALL:81.42%, DEV_F1:80.27%,NO_IMPROVEMENT:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▊        | 100/540 [01:13<04:27,  1.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:100, TRAIN_LOSS:0.086, TRAIN_ACC:100.00%, TRAIN_RECALL:100.00%, TRAIN_F1:100.00%\n",
      " DEV_LOSS:0.257,                        DEV_ACC:90.11%, DEV_RECALL:84.64%, DEV_F1:81.16%,NO_IMPROVEMENT:140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 200/540 [02:32<03:35,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:200, TRAIN_LOSS:0.109, TRAIN_ACC:93.75%, TRAIN_RECALL:81.82%, TRAIN_F1:90.00%\n",
      " DEV_LOSS:0.263,                        DEV_ACC:90.25%, DEV_RECALL:83.99%, DEV_F1:81.26%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 301/540 [04:04<17:58,  4.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITER:300, TRAIN_LOSS:0.103, TRAIN_ACC:93.75%, TRAIN_RECALL:83.33%, TRAIN_F1:83.33%\n",
      " DEV_LOSS:0.258,                        DEV_ACC:89.93%, DEV_RECALL:79.76%, DEV_F1:79.94%,NO_IMPROVEMENT:100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 384/540 [04:54<01:34,  1.64it/s]"
     ]
    }
   ],
   "source": [
    "train(config, model, train_iter, test_iter, data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "uuid": "462441f0-148d-4f54-bfef-fa74e9e082b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "预测完成.输出1679/1680\n",
      "\n",
      "预测完成.输出1679/1680\n",
      "\n",
      "预测完成.输出1679/1680\n",
      "\n",
      "预测完成.输出1679/1680\n",
      "\n",
      "预测完成.输出1679/1680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in range(5):\n",
    "    checkpoint = torch.load('/home/featurize/output/Ernie/checkpoint_cnn{}.pth.tar'.format(i))\n",
    "    model.load_state_dict(checkpoint['state_dict'])\n",
    "    all_pred = predict(config, model, test_iter)\n",
    "    predictions.append(all_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "uuid": "e4a47ba3-b40d-4cc6-ad90-c4dad43df6ad"
   },
   "outputs": [],
   "source": [
    "pred = predictions[0]\n",
    "for i in range(1,5):\n",
    "    pred += predictions[i]\n",
    "pred = (pred/5)\n",
    "pred = np.argmax(pred, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "uuid": "1761d249-8100-4eb5-bd9a-1cf69fad3067"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "submit = pd.read_csv(\"sample_submission.tsv\", header=None, sep=\"\\t\")\n",
    "submit.columns = [\"cid\", \"rid\", \"label\"]\n",
    "submit[\"label\"] = pred\n",
    "submit.to_csv(\"submit_cleaned_new.tsv\", header = None, sep=\"\\t\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "uuid": "1c4fa6ec-ac6a-4487-a83f-67325abbd650"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
